---
title: "Logistic regression tutorial using R"
author: "Leary Ortho Biostats Lab"
date: "12/09/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br/>
<br/>
<br/>

This tutorial will help you understand logistic regression and the ROC curve analysis.

We will firstly explain the data set which we are going to use, and how to import the data to R. Then, a brief introduction of the logistic regression will be given. When to use logistic regression and how to use it in R will be illustrated. Finally, we will show you how to do the ROC curve analysis to measure the performance of the logistic regression.

<br/>

### Data

There are 2000 observations in the data set. The dependent variable is the two-level obesity status of the subject. Subjects who are obese are denoted as "1", and those who are not obese are denoted as "0". The independent variables include age, gender and systolic blood pressure (SBP).

You can download the data in the format of .csv from [Box](https://missouri.box.com/s/0cuco50evri9vadgmhxgemqv8ciktjg0) or [Github](https://raw.githubusercontent.com/zhengyes/logRt/master/tutorial.csv).

<br/>

### Import the data

We highly recommend that you use RStudio for the analysis in this tutorial. RStudio is an integrated development environment (IDE) for R. All the code in this tutorial are case sensitive. 

After downloading the data, you need to find the path of the data file on your computer and import the data to R.

Type the following code in the RStudio console, then a "select file" window will pop up. Use the pop-up window to navigate to the file you want to import and click "Open", the path will display in the console window.

```{r eval=FALSE}
file.choose()
```

Now you have the path of the file, you can use the read.csv() function to read the .csv file into R. Type the following code in the RStudio console but replace the path between quotations, with your file path. The new data set will be named "tutorial".
```{r echo=FALSE}
tutorial = read.csv("C:\\Users\\zs7hm\\Desktop\\tutorial.csv")
```
```{r eval=FALSE}
tutorial = read.csv("C:\\Users\\zs7hm\\Desktop\\tutorial.csv")
```

<br/>

### Logistic regression

<br/>

#### When to use logistic regression
Logistic regression is a statistical model which uses a log function to model a binary dependent variable, or a categorical variable with only two possible outcomes. It is used when we want to predict the binary outcome variable from predictor variables that are continuous and/or categorical. In the data set here, the binary dependent variable is the obesity status, which has "1" means subject is obese and "0" means subject is not obese.

#### How to do logistic regression in R
Because the original data are numbers, R will regard all data as numerical data. However, for obesity and gender variables, the numbers just represent their category levels. In order to tell R about this, we need to change these two variables to categorical ones. We can use the factor() function to do this.

```{r echo=FALSE}
tutorial$gender = factor(tutorial$gender)
tutorial$obesity = factor(tutorial$obesity)
```
```{r eval=FALSE}
tutorial$gender = factor(tutorial$gender)
tutorial$obesity = factor(tutorial$obesity)
```

Now we will use the glm (generalized linear model) function to do the logistic regression. Logistic regression model is an example of the generalized linear model.

In the following code, we build a model called tutorial.model, the dependent variable on the left side of the tilde (~) is the obesity status, and the independent variables are age, gender and systolic blood pressure (sbp), which are on the right side of the tilde (~), joined by plus sign (+). In logistic regression, let p = probability that a subject is obese, we have, $logit(p) = \beta_0 + \beta_1\times age + \beta_2\times gender + \beta_3\times sbp$, the logit function is log odds of obesity which equals to $log(\frac{p}{1-p})$. 

Because we just have 0 and 1 for the outcomes, the distribution of outcome follows a binomial distribution Binomial(n, p), where n is the sample size and p is the probability of being obese. That is the reason why we have family = "binomial" in the code.

"data =tutorial" indicates that we are using the data set "tutorial" for building the model.


```{r eval=FALSE}
tutorial.model=glm(obesity ~ age + gender + sbp, 
                   family = "binomial",
                   data=tutorial)
```
```{r echo=FALSE}
tutorial.model=glm(obesity ~ age + gender + sbp, 
                   family = "binomial",
                   data=tutorial)
```

#### Interpret the result

The model we built is called "titorial.model", it contains all the information for the model. If we want to see the details, we need to run the summary() function to get the results.

```{r eval=FALSE}
# show model details
summary(tutorial.model)
```
```{r echo=FALSE}
# show model details
summary(tutorial.model)
```

Let's see what this result tells us.

**Call:** 

This section tells us how the model is built.

**Deviance Residuals:**

Deviance is a measure of goodness of fit of a generalized linear model. The higher deviance indicates worse model fit. Deviance residual is the measure of deviance contributed from each of the observation. This section tells us the minimum, first quartile, median, third quartile and maximum of the deviance residuals.

**Coefficients:**

---

Note:

For better understanding the result, we need to know what are odds and odds ratio. Odds of an event is defined as the likelihood that an event will occur divided by the likelihood that the event will not occur. Odds ratio is a measure of the variable and the outcome. Odds ratio represents the odds that an outcome will occur given a particular exposure, compared to the odds of the outcome occurring in the absence of that exposure. 

---

The coefficient section shows the estimates of logistic regression coefficients. The "Estimate" column shows the estimations of intercept and coefficients for each variable. The "Std.Error" column shows the standard error of the coefficients. The "z value" column shows the Z statistic, which equals to the value of "Estimate" column divided by the value of "Std.Error" column. The Pr(>|z|) column shows the p-value corresponding to Z statistic, it shows the significance of the estimate. We say a variable is significant when its p-value is less than the significance level (usually 0.05).

With these coefficients, the logistic regression model can be written as $p = \frac{e^{(-3.8658 + 0.0133\times age + 0.4909\times gender + 0.0188\times sbp)}}{1 + e^{(-3.8658 + 0.0133\times age + 0.4909\times gender + 0.0188\times sbp)}}$ (for categorical gender variable, male = 0, female = 1). If we know the age, gender and sbp for a subject, we can predict the probability of being obese by plugging these variable values into the formula.

we can see from the result that all variables are significant with p-values less than 0.05, which means that all model coefficients are significantly different from 0. The coefficient estimation of the categorical variable gender2 equals 0.4910, which means that being female (gender2) increases the log odds of obesity by 0.4910 compared with male (gender1).

The log function and exponential function are inverse functions, so we can use the exponential function to get the odds ratio. Below are the odds ratio and their 95% confidence intervals.
```{r eval=FALSE}
exp(cbind("Odds ratio" = coef(tutorial.model), confint.default(tutorial.model, level = 0.95)))
```
```{r echo=FALSE}
exp(cbind("Odds ratio" = coef(tutorial.model), confint.default(tutorial.model, level = 0.95)))
```

For here, the 95% confidence intervals are all above 1, so different values or levels of the variable will have different associations with outcome variable obesity. The exponential of 0.4910 equals to 1.6340, which means that the odds for female being obese is 63.4% higher than the odds for male being obese. The coefficient estimation of the numerical variable age is 0.0133, which means that one year older in age will increase the log odds of obesity by 0.0133. Exponential of 0.0133 equals to 1.0134, which means that one year older leads to 1.34% higher odds for being obese.

**Null deviance, residual deviance and AIC:**

The null deviance shows how well the outcome variable is predicted by a model that includes only the intercept. The residual deviance shows how well the outcome variable is predicted by the model with all variables included. We can see there are 1999 degree of freedom for the null deviance and 1996 degree of freedom for the residual deviance. This is because 3 degree of freedom are being used for estimating three variables age, gender and sbp. 

AIC (Akaike Information Criterion) is a measure of goodness of fit of the model. We can use it with another model for model performance. Lower AIC value indicates better model fit.

### ROC analysis

<br/>

ROC (Receiver Operating Characteristic) curve displays true positive rate versus false positive rate of a fitted model. It can be used for estimating the performance of a model and selecting the optimal cutoff point.

Before giving you further information about the ROC curve, let us review some vocabulary.

```{r echo=FALSE, results='asis'}
library(knitr)
d=data.frame(
c("True Positive",
"False Positive"),
c("False Negative",
"True Negative"))
row.names(d)=c("Actually Obese","Actually NOT Obese")
colnames(d)=c("Obese Predicted","NOT Obese predicted")
kable(d)
```

* $Sensitivity = \frac{true\  positive}{true\ positive + false\ negative}$, also called true positive rate (TPR).
  
* $1 - specificity = \frac{false\ positive}{false\ positive + true\ negative}$, also called false positive rate (FPR).

In the ROC curve, the y axis denotes the **sensitivity**, which is the true positive rate, and the x axis denotes **1 - specificity** which is the false positive rate. The ROC curve plots out the sensitivity and specificity for every possible cutoff between 0 and 1 for the logistic regression model. The area under the ROC curve is called AUC (area under the curve). The larger the AUC is, the better the model performs.

An optimal cutoff point can be determined by Youden index. The Youden index $$J = sensitivity + specificity -1$$. A value of 1 indicates that there are no false positives or false negatives so that the model is perfect. A value of 0 indicates that true positive rate equals to false positive rate such that the model predicts results no better than by chance. The optimal cutoff point is the one which maximize the Youden index J. Since the coefficients of sensitivity and specificity are both 1, they contribute equally for maximizing the Youden's index, which means that Youden's index is used when we think sensitivity and specificity are equally important.

We need to use the "cutpointr" package to do the ROC analysis. If you do not have this package on your computer, you need to install it firstly, just type the following code in the RStudio console, and the package will be installed automatically.

```{r eval=FALSE}
install.package("cutpointr")
```

The ROC curve is usually a concave curve that towards the upper left corner. The closer the curve is to the corner. The better classification result the model will have. If the curve is a straight line, it means the model has a random classifier and predicts result no better than by chance. The point denoted in the ROC curve figure below corresponds to the optimal Youden's index value. Note, we use the Youden's index when we think sensitivity and specificity are equally important. In the code, x denotes the fitted values and "class" denotes the original dependent binary outcome, obesity.

```{r eval=FALSE}
library(cutpointr)
# create a data frame contains all information about the ROC curve analysis
# x = logistic regression model predicted values for obesity
# class = original values for obesity
cp <- cutpointr(x=tutorial.model$fitted.values, 
                class=tutorial$obesity, 
                metric = youden) 
# plot the ROC curve
plot_roc(cp)
```

```{r,echo=FALSE, message=FALSE}
library(cutpointr)
# create a data frame contains all information about the ROC curve analysis
# x = logistic regression model predicted values for obesity
# class = original values for obesity
cp <- cutpointr(x=tutorial.model$fitted.values, 
                class=tutorial$obesity, 
                metric = youden) 
# plot the ROC curve
plot_roc(cp)
```

The AUC = **0.6884** and it can be used for model comparisons. AUC is the area enclosed by the ROC curve, it has a range between 0.5 and 1. Note, we get AUC = 0.5 when the ROC curve is the straight diagonal line. The higher the AUC value, the better the model is at predicting the obesity.

```{r eval=FALSE}
# display the AUC value
auc(cp)
```
```{r echo=FALSE}
# display the AUC value
auc(cp)
```


The Youden's index shows that the optimal threshold is **0.3164**. 
```{r eval=FALSE}
# show the optimal cut point by maximizing Youden's index
(summary(cp))[[1]][[1]][1,4]
```
```{r echo=FALSE}
# show the optimal cut point by Youden's index
(summary(cp))[[1]][[1]][1,4]
```

<br/>
<br/>
<br/>
