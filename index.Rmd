---
title: "Logistic regression tutorial using R"
author: "Leary Ortho Biostats Lab"
date: "12/03/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br/>
<br/>
<br/>

This tutorial will help you understand logistic regression and the ROC curve analysis.

We will firstly explain the data set which we are going to use, and how to import the data to R. Then, a brief introduction of the logistic regression will be given. When to use logistic regression and how to use it in R will be illustrated. Finally, we will show you how to do the ROC curve analysis to measure the performance of the logistic regression.

<br/>

### Data

There are 2000 observations in the data set. The dependent variable is the two-level obesity status of the subject. Subjects who are obese are denoted as "1", and those who are not obese are denoted as "0". The independent variables include age, gender and systolic blood pressure (SBP).

You can download the data in the format of .csv from [Box](https://missouri.box.com/s/0cuco50evri9vadgmhxgemqv8ciktjg0) or [Github](https://raw.githubusercontent.com/zhengyes/logRt/master/tutorial.csv).

<br/>

### Import the data

We highly recommend that you use RStudio for the analysis in this tutorial. RStudio is an integrated development environment (IDE) for R. All the code in this tutorial are case sensitive. 

After downloading the data, you need to find the path of the data file on your computer and import the data to R.

Type the following code in the RStudio console, then a "select file" window will pop up. Use the pop-up window to navigate to the file you want to import and click "Open", the path will display in the console window.

```{r eval=FALSE}
file.choose()
```

Now you have the path of the file, you can use the read.csv() function to read the .csv file into R. Type the following code in the RStudio console but replace the path between quotations, with your file path. The new data set will be named "tutorial".
```{r echo=FALSE}
tutorial = read.csv("C:\\Users\\zs7hm\\Desktop\\tutorial.csv")
```
```{r eval=FALSE}
tutorial = read.csv("C:\\Users\\zs7hm\\Desktop\\tutorial.csv")
```

<br/>

### Logistic regression

<br/>

#### When to use logistic regression
Logistic regression is a statistical model which uses a log function to model a binary dependent variable, or a categorical variable with only two possible outcomes. It is used when we want to predict the the binary outcome variable from predictor variables that are continuous and/or categorical. In the data set here, the binary dependent variable is the obesity status, which has "1" means subject is obese and "0" means subject is not obese.

#### How to do logistic regression in R
Because the original data are numbers, R will regard all data as numerical data. However, for obesity and gender variables, the numbers just represent their category levels. In order to tell R about this, we need to change these two variables to categorical ones. We can use the factor() function to do this.

```{r echo=FALSE}
tutorial$gender = factor(tutorial$gender)
tutorial$obesity = factor(tutorial$obesity)
```
```{r eval=FALSE}
tutorial$gender = factor(tutorial$gender)
tutorial$obesity = factor(tutorial$obesity)
```

Now we will use the glm (generalized linear model) function to do the logistic regression. Logistic regression model is an example of the generalized linear model.

We build a model called tutorial.model, the dependent variable on the left side of the tilde (~) is the obesity status, and the independent variables are age, gender and systolic blood pressure (sbp), which are on the right side of the tilde (~), joined by plus sign (+). In logistic regression, let p = Probability(subject is obese), we have, logit(p) = $\beta0$ + $\beta1$ * age + $\beta2$ * gender + $\beta3$ * sbp, the logit function is log odds of obesity which equals to log(\frac{p}{1-p}). 

Because we just have 0 and 1 for the outcomes, the distribution of outcome follows a binomial distribution Binomial(n, p) where n is the sample size and p is the probability of being obese. That is the reason why we have family = "binomial" in the code.

"data =tutorial" indicates that we are using the data set "tutorial" for building the model.


```{r eval=FALSE}
tutorial.model=glm(obesity ~ age + gender + sbp, 
                   family = "binomial",
                   data=tutorial)
```
```{r echo=FALSE}
tutorial.model=glm(obesity ~ age + gender + sbp, 
                   family = "binomial",
                   data=tutorial)
```

#### Interpret the result
Let us first explain what is odds. Below is the contingency table for obesity and gender.

```{r echo=FALSE}
tt=tutorial[,c(1,2)]
levels(tt$gender)=c("Male","Female")
levels(tt$obesity)=c("Not Obese","Obese")
table(tt)
```

Odds is the ratio of the probability of "success" over the probability of "failure". In this tutorial, the odds of obesity equal to probability of being obese over probability of not being obese. From the table we can see that, for male the odds of being obese is 265/690 = 0.3840, for female the odds of being obese is 381/664 = 0.5738. The odds ratio of obesity, which is the ratio of odds for female to the odds for male equals to (381/664)/(265/690)= 1.4943, which means that the odds for female being obese have 49.43% higher than the odds for male of being obese in this data set before fitting the model.


In the summary result of this model, we can see that all variables are significant with p-values less than 0.05, which means that all model coefficients are significantly different from 0. The coefficient estimation of gender2 equals 0.4910, which means that being female (gender2) increases the log odds of obesity by 0.4910 compared with male (gender1). The log function and exponential function are inverse functions, so we can use the exponential function to get the odds ratio. For here, exponential of 0.4910 equals to 1.6340, which means that the odds for female being obese is 63.4% higher than the odds for male being obese. The coefficient estimation of age is 0.0133, which means that one year older in age will increase the log odds of obesity by 0.0133. Exponential of 0.0133 equals to 1.0134, which means that one year older leads to 1.34% higher odds for being obese. The coefficient estimation of sbp is 0.0188, which means that one unit increase in systolic blood pressure leads to 0.0188 increase of log odds of obese. Exponetial of 0.0188 equals to 1.0190, so one unit increase in SBP leads to 1.9% higher odds for being obese.


```{r eval=FALSE}
summary(tutorial.model)
exp(tutorial.model$coefficients)
```
```{r echo=FALSE}
summary(tutorial.model)
exp(tutorial.model$coefficients)
```

<br/>

### ROC analysis

<br/>

ROC (Receiver Operating Characteristic) curve displays true positive rate versus false positive rate of a fitted model. It can be used for estimating the performance of a model and selecting the optimal cutoff point.

Before giving you further information about the ROC curve, let us review some vocabulary.

* A true positive means that the model predicts that a subject is obese and the subject is actually obese. 

* A false positive means that the model predicts that a subject is obese but the subject is not obese. 

* A true negative means that the model predicts that the subject is not obese and the subject is not obese. 

* A false negative means that the model predicts that the subject is not obese but the subject is actually obese.

* $Sensitivity = \frac{true\  positive}{true\ positive + false\ negative}$, also called true positive rate (TPR).
  
* $1 - specificity = \frac{false\ positive}{false\ positive + true\ negative}$, also called false positive rate (FPR).

In the ROC curve, the y axis denotes the **sensitivity**, which is the true positive rate, and the x axis denotes **1 - specificity** which is the false positive rate. The ROC curve plots out the sensitivity and specificity for every possible cutoff between 0 and 1 for the logistic regression model. The area under the ROC curve is called AUC (area under the curve). The larger the AUC is, the better the model performs.

An optimal cutoff point can be determined by Youden index. The Youden index $$J = sensitivity + specificity -1$$. A value of 1 indicates that there are no false positives or false negatives so that the model is perfect. A value of zero indicates that true positive rate equals to false positive rate such that the model predicts results no better than by chance. The optimal cutoff point is the one which maximize the Youden index J. Since the coefficients of sensitivity and specificity are both 1, they contribute equally for maximizing the Youden's index, which means that Youden's index is used when we think sensitivity and specificity are equally important.

We need to use the "cutpointr" package to do the ROC analysis. If you do not have this package on your computer, you need to install it firstly, just type the following code in the RStudio console, and the package will be installed automatically.

```{r eval=FALSE}
install.package("cutpointr")
```

The ROC curve is usually a concave curve that towards the upper left corner. The closer the curve is to the corner. The better classification result the model will have. If the curve is a straight line, it means the model has a random classifier and predicts result no better than by chance. The point denoted in the ROC curve figure below corresponds to the optimal Youden's index value. Note, we use the Youden's index when we think sensitivity and specificity are equally important. In the code, x denotes the fitted values and "class" denotes the original dependent binary outcome, obesity.

```{r eval=FALSE}
library(cutpointr)
# create a data frame contains all information about the ROC curve analysis
# x = logistic regression model predicted values for obesity
# class = original values for obesity
cp <- cutpointr(x=tutorial.model$fitted.values, 
                class=tutorial$obesity, 
                metric = youden) 
# plot the ROC curve
plot_roc(cp)
```

```{r,echo=FALSE, message=FALSE}
library(cutpointr)
# create a data frame contains all information about the ROC curve analysis
# x = logistic regression model predicted values for obesity
# class = original values for obesity
cp <- cutpointr(x=tutorial.model$fitted.values, 
                class=tutorial$obesity, 
                metric = youden) 
# plot the ROC curve
plot_roc(cp)
```

The AUC = **0.6884** and it can be used for model comparisons. AUC is the area enclosed by the ROC curve, it has a range between 0.5 and 1. Note, we get AUC = 0.5 when the ROC curve is the straight diagonal line. The higher the AUC value, the better the model is at predicting the obesity.

```{r eval=FALSE}
# display the AUC value
auc(cp)
```
```{r echo=FALSE}
# display the AUC value
auc(cp)
```


The Youden's index shows that the optimal threshold is **0.3164**. 
```{r eval=FALSE}
# show the optimal cut point by maximizing Youden's index
(summary(cp))[[1]][[1]][1,4]
```
```{r echo=FALSE}
# show the optimal cut point by Youden's index
(summary(cp))[[1]][[1]][1,4]
```

<br/>
<br/>
<br/>
